{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nDecoding with SpaceNet: face vs house object recognition\n=========================================================\n\nHere is a simple example of decoding with a SpaceNet prior (i.e Graph-Net,\nTV-l1, etc.), reproducing the Haxby 2001 study on a face vs house\ndiscrimination task.\n\nSee also the SpaceNet documentation: :ref:`space_net`.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Load the Haxby dataset\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn.datasets import fetch_haxby\ndata_files = fetch_haxby()\n\n# Load Target labels\nimport numpy as np\nlabels = np.recfromcsv(data_files.session_target[0], delimiter=\" \")\n\n\n# Restrict to face and house conditions\ntarget = labels['labels']\ncondition_mask = np.logical_or(target == b\"face\", target == b\"house\")\n\n# Split data into train and test samples, using the chunks\ncondition_mask_train = np.logical_and(condition_mask, labels['chunks'] <= 6)\ncondition_mask_test = np.logical_and(condition_mask, labels['chunks'] > 6)\n\n# Apply this sample mask to X (fMRI data) and y (behavioral labels)\n# Because the data is in one single large 4D image, we need to use\n# index_img to do the split easily\nfrom nilearn.image import index_img\nfunc_filenames = data_files.func[0]\nX_train = index_img(func_filenames, condition_mask_train)\nX_test = index_img(func_filenames, condition_mask_test)\ny_train = target[condition_mask_train]\ny_test = target[condition_mask_test]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Fit SpaceNet with a Social sparsity penalty\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn.decoding import SpaceNetClassifier\nfrom nilearn.plotting import plot_stat_map, show\nimport time\n\nfor penalty in ('tv-l1', 'graph-net', 'social'):\n    # Fit model on train data and predict on test data\n    decoder = SpaceNetClassifier(memory=\"nilearn_cache\", penalty='social',\n                                verbose=10,\n                                screening_percentile=50)\n    t0 = time.time()\n    decoder.fit(X_train, y_train)\n    elapsed = time.time() - t0\n    y_pred = decoder.predict(X_test)\n    accuracy = (y_pred == y_test).mean() * 100.\n    title = \"%s; time: %.1fs; accuracy: %g%%\" % (penalty, elapsed, accuracy)\n\n    # Visualization\n    coef_img = decoder.coef_img_\n    display = plot_stat_map(coef_img, data_files.anat[0],\n                            title=title,\n                            cut_coords=(23, -34, -16))\n    display.savefig('haxby_%s.png' % penalty)\n\n    # Save the coefficients to a nifti file\n    coef_img.to_filename('haxby_%s_weights.nii' % penalty)\n\nshow()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can see that the TV-l1 penalty is 3 times slower to converge and\ngives the same prediction accuracy. However, it yields much\ncleaner coefficient maps\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.10", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}