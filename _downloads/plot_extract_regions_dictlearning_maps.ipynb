{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nRegions extraction using Dictionary Learning and functional connectomes\n=======================================================================\n\nThis example shows how to use :class:`nilearn.regions.RegionExtractor`\nto extract spatially constrained brain regions from whole brain maps decomposed\nusing dictionary learning and use them to build a functional connectome.\n\nWe used 20 resting state ADHD functional datasets from :func:`nilearn.datasets.fetch_adhd`\nand :class:`nilearn.decomposition.DictLearning` for set of brain atlas maps.\n\nThis example can also be inspired to apply the same steps to even regions extraction\nusing ICA maps. In that case, idea would be to replace dictionary learning to canonical\nICA decomposition using :class:`nilearn.decomposition.CanICA`\n\nPlease see the related documentation of :class:`nilearn.regions.RegionExtractor`\nfor more details.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The use of the attribute `components_img_` from dictionary learning\n    estimator is implemented from version 0.4.1. For older versions,\n    unmask the deprecated attribute `components_` to get the components\n    image using attribute `masker_` embedded in estimator.\n    See the `section Inverse transform: unmasking data <unmasking_step>`.</p></div>\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Fetch ADHD resting state functional datasets\n---------------------------------------------\n\nWe use nilearn's datasets downloading utilities\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn import datasets\n\nadhd_dataset = datasets.fetch_adhd(n_subjects=20)\nfunc_filenames = adhd_dataset.func\nconfounds = adhd_dataset.confounds"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Extract resting-state networks with DictionaryLearning\n-------------------------------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Import dictionary learning algorithm from decomposition module and call the\n# object and fit the model to the functional datasets\nfrom nilearn.decomposition import DictLearning\n\n# Initialize DictLearning object\ndict_learn = DictLearning(n_components=5, smoothing_fwhm=6.,\n                          memory=\"nilearn_cache\", memory_level=2,\n                          random_state=0)\n# Fit to the data\ndict_learn.fit(func_filenames)\n# Resting state networks/maps in attribute `components_img_`\n# Note that this attribute is implemented from version 0.4.1.\n# For older versions, see the note section above for details.\ncomponents_img = dict_learn.components_img_\n\n# Visualization of resting state networks\n# Show networks using plotting utilities\nfrom nilearn import plotting\n\nplotting.plot_prob_atlas(components_img, view_type='filled_contours',\n                         title='Dictionary Learning maps')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Extract regions from networks\n------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Import Region Extractor algorithm from regions module\n# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n# maps, less the threshold means that more intense non-voxels will be survived.\nfrom nilearn.regions import RegionExtractor\n\nextractor = RegionExtractor(components_img, threshold=0.5,\n                            thresholding_strategy='ratio_n_voxels',\n                            extractor='local_regions',\n                            standardize=True, min_region_size=1350)\n# Just call fit() to process for regions extraction\nextractor.fit()\n# Extracted regions are stored in regions_img_\nregions_extracted_img = extractor.regions_img_\n# Each region index is stored in index_\nregions_index = extractor.index_\n# Total number of regions extracted\nn_regions_extracted = regions_extracted_img.shape[-1]\n\n# Visualization of region extraction results\ntitle = ('%d regions are extracted from %d components.'\n         '\\nEach separate color of region indicates extracted region'\n         % (n_regions_extracted, 5))\nplotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n                         title=title)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Compute correlation coefficients\n---------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# First we need to do subjects timeseries signals extraction and then estimating\n# correlation matrices on those signals.\n# To extract timeseries signals, we call transform() from RegionExtractor object\n# onto each subject functional data stored in func_filenames.\n# To estimate correlation matrices we import connectome utilities from nilearn\nfrom nilearn.connectome import ConnectivityMeasure\n\ncorrelations = []\n# Initializing ConnectivityMeasure object with kind='correlation'\nconnectome_measure = ConnectivityMeasure(kind='correlation')\nfor filename, confound in zip(func_filenames, confounds):\n    # call transform from RegionExtractor object to extract timeseries signals\n    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n    # call fit_transform from ConnectivityMeasure object\n    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n    # saving each subject correlation to correlations\n    correlations.append(correlation)\n\n# Mean of all correlations\nimport numpy as np\nmean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n                                                          n_regions_extracted)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Plot resulting connectomes\n----------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "title = 'Correlation between %d regions' % n_regions_extracted\n\n# First plot the matrix\ndisplay = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n                               colorbar=True, title=title)\n\n# Then find the center of the regions and plot a connectome\nregions_img = regions_extracted_img\ncoords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n\nplotting.plot_connectome(mean_correlations, coords_connectome,\n                         edge_threshold='90%', title=title)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Plot regions extracted for only one specific network\n----------------------------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# First, we plot a network of index=4 without region extraction (left plot)\nfrom nilearn import image\n\nimg = image.index_img(components_img, 4)\ncoords = plotting.find_xyz_cut_coords(img)\ndisplay = plotting.plot_stat_map(img, cut_coords=coords, colorbar=False,\n                                 title='Showing one specific network')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Now, we plot (right side) same network after region extraction to show that\nconnected regions are nicely seperated.\nEach brain extracted region is identified as separate color.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# For this, we take the indices of the all regions extracted related to original\n# network given as 4.\nregions_indices_of_map3 = np.where(np.array(regions_index) == 4)\n\ndisplay = plotting.plot_anat(cut_coords=coords,\n                             title='Regions from this network')\n\n# Add as an overlay all the regions of index 4\ncolors = 'rgbcmyk'\nfor each_index_of_map3, color in zip(regions_indices_of_map3[0], colors):\n    display.add_overlay(image.index_img(regions_extracted_img, each_index_of_map3),\n                        cmap=plotting.cm.alpha_cmap(color))\n\nplotting.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.15", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}