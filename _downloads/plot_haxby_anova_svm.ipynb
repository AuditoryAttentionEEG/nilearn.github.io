{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nThe Haxby dataset: face vs house in object recognition\n=======================================================\n\nThis example does a simple but efficient decoding on the Haxby dataset:\nusing a feature selection, followed by an SVM.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Retrieve the files of the Haxby dataset\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn import datasets\n\nhaxby_dataset = datasets.fetch_haxby_simple()\n\n# print basic information on the dataset\nprint('Mask nifti image (3D) is located at: %s' % haxby_dataset.mask)\nprint('Functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Load the behavioral data\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import numpy as np\ny, session = np.loadtxt(haxby_dataset.session_target[0]).astype(\"int\").T\nconditions = np.recfromtxt(haxby_dataset.conditions_target[0])['f0']\n\n# Restrict to faces and houses\ncondition_mask = np.logical_or(conditions == b'face', conditions == b'house')\ny = y[condition_mask]\nconditions = conditions[condition_mask]\n\n# We have 2 conditions\nn_conditions = np.size(np.unique(y))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Prepare the fMRI data\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn.input_data import NiftiMasker\n\nmask_filename = haxby_dataset.mask\n# For decoding, standardizing is often very important\nnifti_masker = NiftiMasker(mask_img=mask_filename, sessions=session,\n                           smoothing_fwhm=4, standardize=True,\n                           memory=\"nilearn_cache\", memory_level=1)\nfunc_filename = haxby_dataset.func[0]\nX = nifti_masker.fit_transform(func_filename)\n# Apply our condition_mask\nX = X[condition_mask]\nsession = session[condition_mask]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Build the decoder\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Define the prediction function to be used.\n# Here we use a Support Vector Classification, with a linear kernel\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\n\n# Define the dimension reduction to be used.\n# Here we use a classical univariate feature selection based on F-test,\n# namely Anova. We set the number of features to be selected to 500\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfeature_selection = SelectKBest(f_classif, k=500)\n\n# We have our classifier (SVC), our feature selection (SelectKBest), and now,\n# we can plug them together in a *pipeline* that performs the two operations\n# successively:\nfrom sklearn.pipeline import Pipeline\nanova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Fit the decoder and predict\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "anova_svc.fit(X, y)\ny_pred = anova_svc.predict(X)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Visualize the results\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Look at the SVC's discriminating weights\ncoef = svc.coef_\n# reverse feature selection\ncoef = feature_selection.inverse_transform(coef)\n# reverse masking\nweight_img = nifti_masker.inverse_transform(coef)\n\n\n# Create the figure\nfrom nilearn import image\nfrom nilearn.plotting import plot_stat_map, show\n\n# Plot the mean image because we have no anatomic data\nmean_img = image.mean_img(func_filename)\n\nplot_stat_map(weight_img, mean_img, title='SVM weights')\n\n# Saving the results as a Nifti file may also be important\nweight_img.to_filename('haxby_face_vs_house.nii')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Obtain prediction scores via cross validation\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import LeaveOneLabelOut\n\n# Define the cross-validation scheme used for validation.\n# Here we use a LeaveOneLabelOut cross-validation on the session label\n# divided by 2, which corresponds to a leave-two-session-out\ncv = LeaveOneLabelOut(session // 2)\n\n# Compute the prediction accuracy for the different folds (i.e. session)\ncv_scores = []\nfor train, test in cv:\n    anova_svc.fit(X[train], y[train])\n    y_pred = anova_svc.predict(X[test])\n    cv_scores.append(np.sum(y_pred == y[test]) / float(np.size(y[test])))\n\n# Return the corresponding mean prediction accuracy\nclassification_accuracy = np.mean(cv_scores)\n\n# Print the results\nprint(\"Classification accuracy: %.4f / Chance level: %f\" %\n      (classification_accuracy, 1. / n_conditions))\n# Classification accuracy: 0.9861 / Chance level: 0.5000\n\nshow()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.6", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}